{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Linear Regression to Predict House Sale Prices\n",
    "---\n",
    "Written by Neil Mackenzie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This guided project was completed following a Linear Regression [course](https://www.dataquest.io/course/linear-algebra-for-machine-learning/) on Dataquest.io\n",
    "\n",
    "The aim of this project is to practice the implementation of linear regresion to predict the price of houses in Ames, Iowa, United States.\n",
    "\n",
    "The original paper written by Dean De Cock presenting this dataset can be found [here](https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627). The xls version of the dataset can be downloaded directly from [this](http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls) link and information about the columns of the dataset can be found on [this](https://s3.amazonaws.com/dq-content/307/data_description.txt) page. Note the xls file was saved as a csv file for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import display, HTML\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "#Read dataset\n",
    "housing_data = pd.read_csv('AmesHousing.csv', encoding = 'utf-8')\n",
    "#Preview first 5 rows of dataset\n",
    "housing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering\n",
    "The feature columns will be updated in the cells below to handle missing values, transform text and numerical columns and create new columns that reveal valuable information about each house.\n",
    "\n",
    "The strategies for cleaning the dataset and creating new columns are tested below before being implemented into new functions that will carry execute the operations. The operations are outlined below and tested in the cells that follow.\n",
    "1. Drop missing data\n",
    "2. Drop columns containing information that is not valuable to an ML model\n",
    "3. Feature engineering to identify and generate useful information\n",
    "4. Drop columns that are no longer required or leak information about the final sale since this would \"cheat\" the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test operations\n",
    "---\n",
    "The strategies for cleaning the dataset are tested below before being implemented into an ML model. The four steps used to clean and prepare the dataset are:\n",
    "1. Drop missing data\n",
    "2. Drop columns that will not be useful for an ML model\n",
    "3. Feature engineering to derive new columns that reveal valuable information\n",
    "4. Drop columns that leak/reveal information about the sale price of the house since this information will bias the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Drop missing data\n",
    "\n",
    "Missing data will be handled differently depending on the amount of data missing in the column and the type of data stored in the column. The strategy for cleaning columns is described below:\n",
    "1. All columns\n",
    "    - Drop column if > 5% of data in column is missing\n",
    "2. Numerical columns \n",
    "    - Columns containing less than 5% missing data will have missing information filled with the mode of that column\n",
    "3. Text columns\n",
    "    - Drop columns with any missing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview current dataframe to allow confirmation that data is being dropped\n",
    "display(housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Drop columns if > 5% data missing\n",
    "def drop_missing(df):\n",
    "    #Count number of missing values in each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    \n",
    "    #5% cutoff length\n",
    "    cut_off = len(df)*0.05\n",
    "    \n",
    "    #Create df with columns containing <5% missing data\n",
    "    dropped_cols = df.drop(missing_data[missing_data > cut_off].index, axis = 1)\n",
    "    \n",
    "    #Return cleaned df\n",
    "    return dropped_cols\n",
    "\n",
    "missing_dropped = drop_missing(housing_data)\n",
    "\n",
    "#Display updated dataset to check number of cols\n",
    "display(missing_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns has reduced from 82 to 71 so the first step has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Fill missing values for numerical columns if < 5% data missing\n",
    "#Note all columns with >5% missing have already been removed above\n",
    "def fill_na(df):\n",
    "    #Identify numerical columns\n",
    "    numeric_cols = df.select_dtypes(include = np.number)\n",
    "   \n",
    "    #Count missing values in each col and identify columns to fix\n",
    "    count_missing = numeric_cols.isnull().sum()\n",
    "    to_fix = count_missing[count_missing > 0]\n",
    "\n",
    "    #Dictionary of modes for numerical columns that have na values    \n",
    "    replacements = df[to_fix.index].mode().to_dict(orient = 'records')[0]\n",
    "    \n",
    "    dropped_na = df.fillna(replacements)\n",
    "    \n",
    "    return dropped_na\n",
    "\n",
    "missing_numeric = missing_dropped.select_dtypes(include = ['int', 'float']).isnull().sum()\n",
    "missing_numericals = missing_numeric[missing_numeric > 0]\n",
    "print(\"Before removing null values:\")\n",
    "print(missing_numericals)\n",
    "\n",
    "fixed_numeric = fill_na(missing_dropped)\n",
    "fixed_numbers = fixed_numeric.select_dtypes(include = ['int', 'float']).isnull().sum()\n",
    "fixed_numericals = fixed_numbers[fixed_numbers > 0]\n",
    "print(\"\\nAfter removing null values:\")\n",
    "print(fixed_numericals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No numerical columns contain missing data after running the above function. We can now proceed to #3 of the process outlined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Drop columns with missing text data\n",
    "def drop_text(df):\n",
    "    count_text_data = df.select_dtypes(include = ['object']).isnull().sum()\n",
    "    count_text_data.head(10)\n",
    "    \n",
    "    missing_text = count_text_data[count_text_data > 0]\n",
    "    missing_text_index = missing_text.index\n",
    "    missing_text_dropped = df.drop(missing_text_index, axis = 1)\n",
    "    \n",
    "    return missing_text, missing_text_dropped\n",
    "        \n",
    "dropped_cols, cleaned_df = drop_text(fixed_numeric)\n",
    "print(\"Columns that were dropped due to missing text information:\\n\",dropped_cols)\n",
    "print(\"=================================================\")\n",
    "print(\"Cleaned dataframe:\")\n",
    "display(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has been reduced from 71 to 64 columns by removing text columns with missing data. The columns that were dropped are listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop columns that aren't useful for ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the [columns](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) contained in the dataset, it is clear that some columns will not add any value to the ML model. These columns are listed below and removed in the following cell.\n",
    "\n",
    "\n",
    "Column|Description\n",
    "---|---\n",
    "Order|Observation Number\n",
    "PID|Parcel Identification Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ml_cols = ['Order','PID']\n",
    "non_ml_removed = cleaned_df.drop(non_ml_cols, axis = 1)\n",
    "\n",
    "#print preview of df to confirm columns were dropped. Number of cols should be 2 less than df above\n",
    "display(non_ml_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n",
    "New columns will now be created to quickly identify some information about the houses which is not immediately clear in the dataset. This includes identifying how old the house was when it was sold and how many years passed after the house was renovated before it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new columns\n",
    "non_ml_removed['Age At Sale'] = non_ml_removed['Yr Sold'] - non_ml_removed['Year Built']\n",
    "non_ml_removed['Yrs Between Remod & Sale'] = non_ml_removed['Yr Sold'] - non_ml_removed['Year Remod/Add']\n",
    "\n",
    "#Check for values less than 0 in new columns (these represent errors):\n",
    "display(non_ml_removed[non_ml_removed['Age At Sale'] < 0])\n",
    "display(non_ml_removed[non_ml_removed['Yrs Between Remod & Sale'] < 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows shown above have some sort of error since the house could not have been sold before it was built or renovated. These rows will therefore be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_non_ml_removed = non_ml_removed.drop([1702, 2180, 2181])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop columns that leak final price information\n",
    "Columns that may leak information were also determined by reviewing the columns of the dataset on the same webpage listed above. These columns are removed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_leak_cols = ['Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold']\n",
    "leaked_dropped = cleaned_non_ml_removed.drop(drop_leak_cols, axis = 1)\n",
    "\n",
    "display(leaked_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model Development\n",
    "The data cleaning and feature engineering strategy was determined above and can now be implemented into functions that execute all of the operationstested above. The dataset will then be split into test and training sets which will be used to train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transform_features() to execture cleaning operations 1 to 4 from above\n",
    "def transform_features(df):\n",
    "    \n",
    "    #Create df with <5% missing values in each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    df = df.drop(missing_data[missing_data > len(df)*0.05].index, axis = 1)\n",
    "    \n",
    "    #Fill missing values for numerical columns\n",
    "    num_missing = df.select_dtypes(include = np.number).isnull().sum()\n",
    "    num_to_fix = num_missing[num_missing > 0]\n",
    "    replacements = df[num_to_fix.index].mode().to_dict(orient = 'records')[0]\n",
    "    df = df.fillna(replacements)\n",
    "    \n",
    "    #Remove text columns if any values missing\n",
    "    count_text_data = df.select_dtypes(include = ['object']).isnull().sum()    \n",
    "    missing_text = count_text_data[count_text_data > 0]\n",
    "    df = df.drop(missing_text.index, axis = 1)\n",
    "    \n",
    "    #Create new columns\n",
    "    df['Age At Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    df['Yrs Between Remod & Sale'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    \n",
    "    #Drop non-ml and columns and columns that leak price info\n",
    "    drop_cols = ['Order','PID','Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold']\n",
    "    df = df.drop(drop_cols, axis = 1)\n",
    "    \n",
    "    #Drop columns that were found to have false values after feature engineering\n",
    "    df = df.drop([1702, 2180, 2181])\n",
    "    \n",
    "    #Return cleaned and engineered dataset\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract training set\n",
    "def get_datasets(df,length):\n",
    "    #Create new train and test dataframes to keep original df in tact\n",
    "    train = df.iloc[0:length]\n",
    "    test = df.iloc[length:]\n",
    "    return train, test\n",
    "\n",
    "#Function to select features for training\n",
    "#This is just a placeholder at this stage. To be developed in next section\n",
    "def select_features(df, columns):\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train and test a LR model\n",
    "def train_and_test(df):\n",
    "    #Length of training dataset\n",
    "    training_length = 1460\n",
    "    #Isolate training and testing datasets\n",
    "    train, test = get_datasets(df,training_length)\n",
    "    \n",
    "    #Numericalcolumns to use for training model\n",
    "    numeric_train = train.select_dtypes(include = np.number)\n",
    "    train_features = numeric_train.columns.drop('SalePrice')\n",
    "    numeric_test = test.select_dtypes(include = np.number)\n",
    "    test_features = numeric_test.columns.drop('SalePrice')\n",
    "    \n",
    "    #Dataframes of taining, target and test data\n",
    "    train_data = select_features(train,train_features)\n",
    "    train_target = select_features(train,['SalePrice'])\n",
    "    test_data = select_features(test,test_features)\n",
    "\n",
    "    #Fit linear regression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train_data,train_target)\n",
    "    #Predict target column using model trained on selected columns\n",
    "    predictions = lr.predict(test_data)\n",
    "    \n",
    "    #Calculate mse and rmse\n",
    "    mse = mean_squared_error(test['SalePrice'], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "    \n",
    "#Clean and transform columns in dataset    \n",
    "transformed_df = transform_features(housing_data)\n",
    "\n",
    "#Get RMSE value for cleaned dataset using all columns to predict value\n",
    "rmse = train_and_test(transformed_df)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset has been cleaned and transformed and a model has been made to determined the RMSE value for a given set of columns, its time to move on to feature selection. This will identify the best columns to use for predicting the sale price and will be done in two steps which are described below\n",
    "\n",
    "1. The correlation between each numerical column and the target column (SalePrice) will be assessed with a correlation matrix.\n",
    "2. Columns that should be encoded as categorical will be converted into the correct type using [dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "---\n",
    "## 1. Correlation\n",
    "The correlation between each feature and the target column is assessed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify target values\n",
    "target = transformed_df['SalePrice']\n",
    "\n",
    "#Extract only numerical columns\n",
    "numerical_cols = transformed_df.select_dtypes(include = [np.number])\n",
    "\n",
    "#Check correlation of each feature with SalePrice column\n",
    "correlation_mat = numerical_cols.corr()['SalePrice'].sort_values()\n",
    "\n",
    "#print correlation matrix\n",
    "with pd.option_context('display.max_rows', 40):\n",
    "    display(correlation_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values in the cell above indicate a negative correlation with sale price. This means that 'Age at Sale' has a strong negative correlation with the sale price. This makes intuitive sense since it would be expected that an older house would be sold for a lower price.\n",
    "\n",
    "This information can be used to drop columns with correlation coefficients below a selected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Categorical Columns\n",
    "The features that should be categorical are identified and handled below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns that should be categorical from original dataset:\n",
    "all_cat_cols = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "            \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "            \"Exterior 2nd\", \"Exter Cond\", \"Exter Qual\",  \"Mas Vnr Type\", \"Foundation\", \"Functional\", \"Heating\", \n",
    "                \"Central Air\", \"Garage Type\", \"Misc Feature\", \"Sale Type\", \"Sale Condition\", \"Heating QC\",\n",
    "                \"Kitchen Qual\", \"Land Slope\", \"Lot Shape\", \"Paved Drive\", \"Utilities\"]\n",
    "\n",
    "#Categorical columns that are still in cleaned, transformed dataset:\n",
    "cat_cols = []\n",
    "for col in all_cat_cols:\n",
    "    if col in transformed_df.columns:\n",
    "        cat_cols.append(col)\n",
    "\n",
    "#Check categorical columns in dataset as determined above\n",
    "cat_cols.sort()\n",
    "print(cat_cols,'\\n')\n",
    "\n",
    "#Confirm categorical columns by comparing the list generated above with 'object' columns\n",
    "text_cols = transformed_df.select_dtypes(include = ['object']).columns.tolist()\n",
    "print(sorted(text_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns shown above should contain categorical values. The 'MS SubClass' column was not picked up in the test at the end of the cell block since this column contains numerical values that are used to categorize the type of dwelling involved. This column should therefore still be included as a 'category' column.\n",
    "\n",
    "Dummy coding will now be used below to separate each of these columns into n binary columns where n is the number of unique values in the column. \n",
    "\n",
    "Doing this for all columns irrespective of the number of unique values could make a massive dataframe so lets first check the number of unique values in each of these columns. Dummy coding will be used for columns that contain fewer than 10 unique values. Columns with more than 10 unique values will be removed to avoid making the dataframe too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of unique values in each categorical column\n",
    "unique_vals = transformed_df[cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "\n",
    "# Extract index of categorical columns with less than 10 unique values and remove columns with over 10 unique vals\n",
    "unique_under_10 = unique_vals[unique_vals < 10].index\n",
    "unique_over_10 = unique_vals[unique_vals >= 10].index\n",
    "reduced_df = transformed_df.drop(unique_over_10, axis = 1)\n",
    "\n",
    "# Convert categorical columns to 'category' datatype for use with get_dummies function\n",
    "cat_cols = transformed_df[unique_under_10].copy()\n",
    "for col in cat_cols:\n",
    "    cat_cols[col] = cat_cols[col].astype('category')\n",
    "\n",
    "# Create df with numiercal columns and dummy columns in place of categorical columns\n",
    "df_with_cats = pd.concat([transformed_df, pd.get_dummies(cat_cols)], axis = 1)\n",
    "df_with_cats = df_with_cats.drop(unique_under_10, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cut-off number of unique feature values was chosen as 10 in the example above but can be pre-defined to avoid the dataset becoming too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "The linear regression model from the section above can now be updated to include the feature selection techniques tested above. The LR model from above is copy-pasted below with the changes incorporated into the select_features function. \n",
    "\n",
    "The parameter 'k' is also introducted to the train_and_test function for k-fold cross validation. This requires if statements to be added to the function which alter the behaviour depending on the value of k since k-fold cross validation does not work for k = 0 ([holdout validation](https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f)) or k = 1 (simple cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transform_features() to execture cleaning operations 1 to 4 from above\n",
    "def transform_features(df):\n",
    "    \n",
    "    #Create df with <5% missing values in each column\n",
    "    missing_data = df.isnull().sum()\n",
    "    df = df.drop(missing_data[missing_data > len(df)*0.05].index, axis = 1)\n",
    "    \n",
    "    #Fill missing values for numerical columns\n",
    "    num_missing = df.select_dtypes(include = np.number).isnull().sum()\n",
    "    num_to_fix = num_missing[num_missing > 0]\n",
    "    replacements = df[num_to_fix.index].mode().to_dict(orient = 'records')[0]\n",
    "    df = df.fillna(replacements)\n",
    "    \n",
    "    #Remove text columns if any values missing\n",
    "    count_text_data = df.select_dtypes(include = ['object']).isnull().sum()    \n",
    "    missing_text = count_text_data[count_text_data > 0]\n",
    "    df = df.drop(missing_text.index, axis = 1)\n",
    "    \n",
    "    #Create new columns\n",
    "    df['Age At Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    df['Yrs Between Remod & Sale'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    \n",
    "    #Drop non-ml and columns and columns that leak price info\n",
    "    drop_cols = ['Order','PID','Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold']\n",
    "    df = df.drop(drop_cols, axis = 1)\n",
    "    \n",
    "    #Drop columns that were found to have false values after feature engineering\n",
    "    df = df.drop([1702, 2180, 2181])\n",
    "    \n",
    "    #Return cleaned and engineered dataset\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract training set\n",
    "def get_datasets(df,length):\n",
    "    #Create new train and test dataframes to keep original df in tact\n",
    "    train = df.iloc[0:length]\n",
    "    test = df.iloc[length:]\n",
    "    return train, test\n",
    "\n",
    "#Function to select features for training\n",
    "def select_features(df, corr_cutoff, dummy_cutoff):\n",
    "\n",
    "#Drop numerical features with correlation less than corr_cutoff\n",
    "    target = df['SalePrice']\n",
    "\n",
    "    #Extract only numerical columns\n",
    "    numerical_cols = df.select_dtypes(include = [np.number])\n",
    "\n",
    "    #Check correlation of each feature with SalePrice column\n",
    "    corr_mat = numerical_cols.corr()['SalePrice'].sort_values()\n",
    "    \n",
    "    #Drop values with correlation lower than corr_cutoff\n",
    "    df = df.drop(corr_mat[corr_mat < corr_cutoff].index, axis = 1)\n",
    "\n",
    "\n",
    "# Convert categorical columns to dummy columns after dropping those with more than dummy_cutoff unique values\n",
    "\n",
    "    #Columns that should be categorical from original dataset:\n",
    "    all_cat_cols = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Exter Cond\", \"Exter Qual\",  \"Mas Vnr Type\", \"Foundation\", \"Functional\", \"Heating\", \n",
    "                    \"Central Air\", \"Garage Type\", \"Misc Feature\", \"Sale Type\", \"Sale Condition\", \"Heating QC\",\n",
    "                    \"Kitchen Qual\", \"Land Slope\", \"Lot Shape\", \"Paved Drive\", \"Utilities\"]\n",
    "\n",
    "    #Categorical columns that are still in cleaned, transformed dataset:\n",
    "    cat_cols = []\n",
    "    for col in all_cat_cols:\n",
    "        if col in df.columns:\n",
    "            cat_cols.append(col)\n",
    "\n",
    "    # Check for number of unique values in each categorical column\n",
    "    unique_vals = df[cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "\n",
    "    # Extract index of categorical columns with less than 10 unique values and remove columns with over 10 unique vals\n",
    "    unique_under_cutoff = unique_vals[unique_vals < dummy_cutoff].index\n",
    "    unique_over_cutoff = unique_vals[unique_vals >= dummy_cutoff].index\n",
    "    reduced_df = df.drop(unique_over_cutoff, axis = 1)\n",
    "\n",
    "    # Convert categorical columns to 'category' datatype for use with get_dummies function\n",
    "    cat_cols = transformed_df[unique_under_10].copy()\n",
    "    for col in cat_cols:\n",
    "        cat_cols[col] = cat_cols[col].astype('category')\n",
    "\n",
    "    # Create df with numerical columns and dummy columns with original categorical columns removed    df_with_cats = pd.concat([transformed_df, pd.get_dummies(cat_cols)], axis = 1)\n",
    "    df_final = df_with_cats.drop(unique_over_cutoff, axis = 1)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train and test a LR model. k = 0 by default\n",
    "def train_and_test(df, k=0 ):\n",
    "    #Isolate numeric features:\n",
    "    numeric_features = df.select_dtypes(include = [np.number])\n",
    "    features = numeric_features.columns.drop('SalePrice')\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    #holdout validation\n",
    "    if k == 0:\n",
    "        #Length of training dataset\n",
    "        training_length = 1460\n",
    "        #Isolate training and testing datasets\n",
    "        train, test = get_datasets(df,training_length)\n",
    "\n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        return predictions, rmse\n",
    "    \n",
    "    if k == 1:\n",
    "         # Randomize all rows ie frac = 1\n",
    "        shuffled_df = df.sample(frac = 1)\n",
    "        #Length of training dataset\n",
    "        training_length = 1460\n",
    "        #Isolate training and testing datasets\n",
    "        train, test = get_datasets(df,training_length)\n",
    "        \n",
    "        #First model - train on first half and test on seconds half\n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions_one = lr.predict(test[features])        \n",
    "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        \n",
    "        #Second model - train on seconds half and train on first half\n",
    "        lr.fit(test[features], test[\"SalePrice\"])\n",
    "        predictions_two = lr.predict(train[features])        \n",
    "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
    "        rmse_two = np.sqrt(mse_two)\n",
    "        \n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        print(\"RMSE training on first half:\",rmse_one)\n",
    "        print(\"RMSE training on seconds half:\",rmse_two)\n",
    "        print(\"Average RMSE:\", avg_rmse)\n",
    "        return predictions_one, avg_rmse\n",
    "    \n",
    "    else:\n",
    "        #Kfold cross validation using n splits\n",
    "        #Use random_state argument for reproducible results\n",
    "        kf = KFold(n_splits = k, shuffle = True, random_state = 3)\n",
    "        rmse_values = []\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            #Length of training dataset\n",
    "            training_length = 1460\n",
    "\n",
    "            #Dataframes of taining, target and test data\n",
    "            train_data = df.iloc[train_index]\n",
    "            test_data = df.iloc[test_index]\n",
    "\n",
    "            #Fit linear regression model\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(train_data[features],train_data['SalePrice'])\n",
    "            #Predict target column using model trained on selected columns\n",
    "            predictions = lr.predict(test_data[features])\n",
    "\n",
    "            #Calculate mse and rmse\n",
    "            mse = mean_squared_error(test_data['SalePrice'], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "            \n",
    "        #Print actual vs predicted value\n",
    "        print('Preview of actual prices vs predicted results')\n",
    "        rounded = [elem.astype('int') for elem in predictions]\n",
    "        data = {\"SalePrice\": test_data['SalePrice'], \"Predicted Value\":rounded}\n",
    "        results = pd.DataFrame(data)\n",
    "        display(results)\n",
    "        \n",
    "        #Print rmse values and plot results\n",
    "        print(\"RMSE Values for K-fold cross validation:\")\n",
    "        print(rmse_values)\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        plt.scatter(x=results['SalePrice'], y = results['Predicted Value'])\n",
    "        plt.yticks(np.arange(100000,1000000,100000))\n",
    "        plt.xlabel('Actual Sale Price', fontsize = 14)\n",
    "        plt.ylabel('Predicted Sale Price', fontsize = 14)\n",
    "        plt.title('Actual vs Predicted Sale Price', fontsize = 18)\n",
    "            \n",
    "        return predictions, np.mean(rmse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run transform_features() to clean dataset\n",
    "transformed_df = transform_features(housing_data)\n",
    "\n",
    "#Run select_features() to perform feature selection\n",
    "filtered_df = select_features(transformed_df, 0.4, 10)\n",
    "\n",
    "#Train and test KNN model\n",
    "values, rmse = train_and_test(filtered_df, k = 4)\n",
    "\n",
    "print(\"length\",len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=================================================\")\n",
    "print(\"Average RMSE Value:\",int(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Linear regression was used to predict the sale price of houses in Ames, Iowa, USA. \n",
    "\n",
    "The operations to perform before training a linear regression model were first tested by dropping/filling missing data, dropping columns that would not be valuable to the model, performing feature engineering to produce new, meaningful columns and dropping columns that would reveal the final sale price of the house.\n",
    "\n",
    "The linear regression model was then developed using K-Fold cross validation to estimate the skill of the model.\n",
    "\n",
    "The cleaned dataset of 2927 rows was split into training and testing sets with the training set containing 1460 rows and the testing set containing the remainder.\n",
    "\n",
    "The actual vs predicted sale price are shown in the chart below for a linear regression model using 4-fold cross validation \n",
    "\n",
    "<img src = \"results.jpg\">\n",
    "\n",
    "The scatter plot above shows strongly linear results which indicate that the model accurately predicts the sale price of houses in Ames using the selected features. The average RMSE value for this model was 28 447."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
